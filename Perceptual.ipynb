{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "\n",
    "import copy\n",
    "\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.is_available()) \n",
    "device = torch.device( \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if (torch.cuda.is_available()):\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Image Transform Network Blocks as defined in Perceptual Losses for Real-Time Style Transfer and Super-Resolution\"\"\"\n",
    "\"\"\"This part of the code is adapted from https://github.com/dxyang/StyleTransfer\"\"\"\n",
    "# Conv Layer\n",
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride) #, padding)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "class UpsampleConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        if upsample:\n",
    "            self.upsample = nn.Upsample(scale_factor=upsample, mode='nearest')\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.upsample:\n",
    "            x = self.upsample(x)\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        out = self.relu(out)\n",
    "        return out \n",
    "\n",
    "# Image Transform Network\n",
    "class ImageTransformNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImageTransformNet, self).__init__()\n",
    "        \n",
    "        # nonlineraity\n",
    "        self.relu = nn.ReLU()\n",
    "        self.tanh = nn.Tanh()\n",
    "\n",
    "        # encoding layers\n",
    "        self.conv1 = ConvLayer(3, 32, kernel_size=9, stride=1)\n",
    "        self.in1_e = nn.InstanceNorm2d(32, affine=True)\n",
    "\n",
    "        self.conv2 = ConvLayer(32, 64, kernel_size=3, stride=2)\n",
    "        self.in2_e = nn.InstanceNorm2d(64, affine=True)\n",
    "\n",
    "        self.conv3 = ConvLayer(64, 128, kernel_size=3, stride=2)\n",
    "        self.in3_e = nn.InstanceNorm2d(128, affine=True)\n",
    "\n",
    "        # residual layers\n",
    "        self.res1 = ResidualBlock(128)\n",
    "        self.res2 = ResidualBlock(128)\n",
    "        self.res3 = ResidualBlock(128)\n",
    "        self.res4 = ResidualBlock(128)\n",
    "        self.res5 = ResidualBlock(128)\n",
    "\n",
    "        # decoding layers\n",
    "        self.deconv3 = UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2 )\n",
    "        self.in3_d = nn.InstanceNorm2d(64, affine=True)\n",
    "\n",
    "        self.deconv2 = UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2 )\n",
    "        self.in2_d = nn.InstanceNorm2d(32, affine=True)\n",
    "\n",
    "        self.deconv1 = UpsampleConvLayer(32, 3, kernel_size=9, stride=1)\n",
    "        self.in1_d = nn.InstanceNorm2d(3, affine=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        y = self.relu(self.in1_e(self.conv1(x)))\n",
    "        y = self.relu(self.in2_e(self.conv2(y)))\n",
    "        y = self.relu(self.in3_e(self.conv3(y)))\n",
    "\n",
    "        # residual layers\n",
    "        y = self.res1(y)\n",
    "        y = self.res2(y)\n",
    "        y = self.res3(y)\n",
    "        y = self.res4(y)\n",
    "        y = self.res5(y)\n",
    "\n",
    "        # decode\n",
    "        y = self.relu(self.in3_d(self.deconv3(y)))\n",
    "        y = self.relu(self.in2_d(self.deconv2(y)))\n",
    "        #y = self.tanh(self.in1_d(self.deconv1(y)))\n",
    "        y = self.deconv1(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"propagation_layers = ['relu_1', 'relu_2', 'relu_3', 'relu_4']\\nclass Vgg19(nn.Module):\\n    def __init__(self):\\n        super(Vgg19, self).__init__()\\n        features = models.vgg19(pretrained=True).features\\n        self.layers = []\\n        for i in propagation_layers:\\n            self.layers.append( nn.Sequential() )\\n        \\n        checkpoints = [4,9,16,23]\\n        layer_index = 0\\n        for i in range(checkpoints[-1]):\\n            if i in checkpoints:\\n                layer_index += 1\\n            self.layers[layer_index].add_module(str(i), features[i])\\n            \\n        for param in self.parameters():\\n            param.requires_grad = False\\n\\n    def forward(self, image):\\n        print(self.layers)\\n        state_vector = []\\n        inter = self.layers[0](image)\\n        state_vector.append(inter)\\n        for i in range(1,len(self.layers)):\\n            inter = self.layers[i](inter)\\n            state_vector.append(inter)\\n        return state_vector\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"propagation_layers = ['relu_1', 'relu_2', 'relu_3', 'relu_4']\n",
    "class Vgg19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg19, self).__init__()\n",
    "        features = models.vgg19(pretrained=True).features\n",
    "        self.layers = []\n",
    "        for i in propagation_layers:\n",
    "            self.layers.append( nn.Sequential() )\n",
    "        \n",
    "        checkpoints = [4,9,16,23]\n",
    "        layer_index = 0\n",
    "        for i in range(checkpoints[-1]):\n",
    "            if i in checkpoints:\n",
    "                layer_index += 1\n",
    "            self.layers[layer_index].add_module(str(i), features[i])\n",
    "            \n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, image):\n",
    "        print(self.layers)\n",
    "        state_vector = []\n",
    "        inter = self.layers[0](image)\n",
    "        state_vector.append(inter)\n",
    "        for i in range(1,len(self.layers)):\n",
    "            inter = self.layers[i](inter)\n",
    "            state_vector.append(inter)\n",
    "        return state_vector\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vgg19(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Vgg19, self).__init__()\n",
    "        features = models.vgg16(pretrained=True).features\n",
    "        self.to_relu_1_2 = nn.Sequential() \n",
    "        self.to_relu_2_2 = nn.Sequential() \n",
    "        self.to_relu_3_3 = nn.Sequential()\n",
    "        self.to_relu_4_3 = nn.Sequential()\n",
    "\n",
    "        for x in range(4):\n",
    "            self.to_relu_1_2.add_module(str(x), features[x])\n",
    "        for x in range(4, 9):\n",
    "            self.to_relu_2_2.add_module(str(x), features[x])\n",
    "        for x in range(9, 16):\n",
    "            self.to_relu_3_3.add_module(str(x), features[x])\n",
    "        for x in range(16, 23):\n",
    "            self.to_relu_4_3.add_module(str(x), features[x])\n",
    "        \n",
    "        # don't need the gradients, just want the features\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.to_relu_1_2(x)\n",
    "        h_relu_1_2 = h\n",
    "        h = self.to_relu_2_2(h)\n",
    "        h_relu_2_2 = h\n",
    "        h = self.to_relu_3_3(h)\n",
    "        h_relu_3_3 = h\n",
    "        h = self.to_relu_4_3(h)\n",
    "        h_relu_4_3 = h\n",
    "        out = (h_relu_1_2, h_relu_2_2, h_relu_3_3, h_relu_4_3)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_imagenet():\n",
    "    return transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "width = 512 if torch.cuda.is_available() else 256\n",
    "height = 512  if torch.cuda.is_available() else 256\n",
    "common_size = 512 if torch.cuda.is_available() else 256\n",
    "\n",
    "convert = transforms.ToPILImage()\n",
    "\n",
    "transformer = transforms.Compose([\n",
    "            transforms.Resize((common_size,common_size)), \n",
    "            transforms.CenterCrop(common_size),\n",
    "            transforms.ToTensor(),             \n",
    "            normalize_imagenet()     \n",
    "    ])\n",
    "\n",
    "def load_image(image_name):\n",
    "    orig_image = Image.open(image_name)\n",
    "    return orig_image\n",
    "\n",
    "def transform( orig_image ):\n",
    "    image = transformer(orig_image).unsqueeze(0).to(device, torch.float)\n",
    "    if (image.size())[1] != 3:\n",
    "        image = torch.cat([image,image,image], dim=1) \n",
    "    return image\n",
    "\n",
    "def display_image( image ):\n",
    "    display = image.cpu().clone()\n",
    "    display = display.squeeze(0)\n",
    "    display = convert(display)\n",
    "    print(display.size)\n",
    "    plt.imshow(display)\n",
    "\n",
    "def save_image(filename, data):\n",
    "    std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))\n",
    "    mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))\n",
    "    img = data.clone().numpy()\n",
    "    img = ((img * std + mean).transpose(1, 2, 0)*255.0).clip(0, 255).astype(\"uint8\")\n",
    "    img = Image.fromarray(img)\n",
    "    img.save(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_content_loss( y_c_features, y_hat_features):\n",
    "    loss_mse = torch.nn.MSELoss()\n",
    "    return loss_mse(y_c_features[1] , y_hat_features[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(image):\n",
    "    (b, c, h, w) = image.size()\n",
    "    features = image.view(b, c, w * h)\n",
    "    features_t = features.transpose(1, 2)\n",
    "    G = features.bmm(features_t) / (c * h * w)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_style_loss(y_hat_features, style_gram):\n",
    "    y_hat_gram = [gram_matrix(fmap) for fmap in y_hat_features]\n",
    "    \n",
    "    loss_mse = torch.nn.MSELoss()\n",
    "    \n",
    "    style_loss = 0.0\n",
    "    for j in range(len(style_gram)):\n",
    "        style_loss += loss_mse(y_hat_gram[j], style_gram[j])\n",
    "    return style_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "def Optimizer(params):\n",
    "    optimizer = optim.Adam(params, lr) #Optimizer for CNN (Try with LBFGS)\n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "\n",
    "dataset_transform = transforms.Compose([\n",
    "    transforms.Resize((common_size,common_size)),          \n",
    "    transforms.ToTensor(),                \n",
    "    normalize_imagenet()     \n",
    "])\n",
    "\n",
    "style_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize_imagenet()     \n",
    "])\n",
    "\n",
    "def transfer_style(content_image_path, style_image_path, model_path, output,\n",
    "                       epoch=2, lamda_style=10000, lamda_content=1 ):\n",
    "    try: \n",
    "        style_model = ImageTransformNet().type(torch.cuda.FloatTensor)\n",
    "        style_model.load_state_dict(torch.load(model_path))\n",
    "        \n",
    "        content_image = load_image(content_image_path)\n",
    "        content_image = transformer(content_image)\n",
    "        content_image = content_image.unsqueeze(0)\n",
    "        content_image = Variable(content_image).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        stylized = style_model(content_image).cpu()\n",
    "        save_image(output, stylized.data[0])\n",
    "    except:\n",
    "        print(\"No model found, switched to model training\")\n",
    "        image_transformer = ImageTransformNet().type(torch.cuda.FloatTensor)\n",
    "        optimizer = Optimizer(image_transformer.parameters())\n",
    "        \n",
    "        vgg = Vgg19().type(torch.torch.cuda.FloatTensor)\n",
    "\n",
    "        train_dataset = datasets.ImageFolder(dataset, dataset_transform)\n",
    "        train_loader = DataLoader(train_dataset, batch_size = batch_size)\n",
    "\n",
    "        style = load_image(style_image_path)\n",
    "        style = style_transform(style)\n",
    "        style = Variable(style.repeat(batch_size, 1, 1, 1)).type(torch.cuda.FloatTensor)\n",
    "\n",
    "        style_features = vgg(style)\n",
    "        style_gram = [gram_matrix(fmap) for fmap in style_features]\n",
    "\n",
    "        for e in range(epoch):\n",
    "            image_transformer.train()\n",
    "            for batch_num, (x, label) in enumerate(train_loader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                x = Variable(x).type(torch.cuda.FloatTensor)\n",
    "                y_hat = image_transformer(x)\n",
    "\n",
    "                y_c_features = vgg(x)\n",
    "                y_hat_features = vgg(y_hat)\n",
    "                \n",
    "                style_loss = calc_style_loss( y_hat_features, style_gram )\n",
    "                content_loss = calc_content_loss( y_c_features, y_hat_features)\n",
    "\n",
    "                loss = lamda_style*style_loss + lamda_content*content_loss \n",
    "                print(batch_num)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "        image_transformer.eval()\n",
    "        image_transformer.cpu()\n",
    "        \n",
    "        model_name = \"pretrained_model\"\n",
    "\n",
    "        filename = str(model_name) + \".model\"\n",
    "        torch.save(image_transformer.state_dict(), filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time 0.601315975189209 ms\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "#Run twice or more by changing the content_image\n",
    "dataset = \"./train_coco_big\"\n",
    "model_path = \"./models/mosaic.model\"\n",
    "output = \"out.png\"\n",
    "style_image_path = \"./images/style_images/mosaic.jpeg\"\n",
    "content_image_path = \"./images/content_images/uni2.jpeg\"\n",
    "transfer_style(content_image_path, style_image_path, model_path, output)\n",
    "toc = time.time()\n",
    "elapsed = toc - tic\n",
    "print(\"Elapsed time {} second\".format(elapsed))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
